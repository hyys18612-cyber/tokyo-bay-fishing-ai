import tweepy
import os
import datetime
import pandas as pd
import joblib
import requests
import numpy as np
import json
import time
from requests_oauthlib import OAuth1
from geopy.geocoders import Nominatim
import warnings
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.font_manager as fm

warnings.filterwarnings('ignore')

# ==========================================
# 1. èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿
# ==========================================
consumer_key = os.environ.get("TWITTER_API_KEY")
consumer_secret = os.environ.get("TWITTER_API_SECRET")
access_token = os.environ.get("TWITTER_ACCESS_TOKEN")
access_token_secret = os.environ.get("TWITTER_ACCESS_TOKEN_SECRET")

# ==========================================
# 2. X API v2 æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢æ•° (å¼·åŒ–ç‰ˆ)
# ==========================================
def upload_media_v2(filename, consumer_key, consumer_secret, access_token, access_token_secret):
    """
    Tweepyã‚’ä½¿ã‚ãšã€requestsã§ç›´æ¥API v2ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å©ã„ã¦ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    (å¾…æ©Ÿå‡¦ç†å¼·åŒ–ç‰ˆ)
    """
    auth = OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)
    file_size = os.path.getsize(filename)
    
    url = "https://upload.twitter.com/1.1/media/upload.json"
    
    # --- Step 1: INIT ---
    init_data = {
        "command": "INIT",
        "total_bytes": file_size,
        "media_type": "image/png",
        "media_category": "tweet_image"
    }
    
    print("ğŸ“¤ [v2] Upload Step 1: INIT")
    res_init = requests.post(url, data=init_data, auth=auth)
    
    if res_init.status_code not in [200, 202]:
        print(f"âŒ INIT Failed: {res_init.text}")
        raise Exception(f"Media Upload INIT Failed: {res_init.status_code}")
        
    media_id = res_init.json()['media_id_string']
    print(f"   Media ID Issued: {media_id}")
    
    # --- Step 2: APPEND ---
    print(f"ğŸ“¤ [v2] Upload Step 2: APPEND")
    
    with open(filename, 'rb') as f:
        segment_id = 0
        while True:
            chunk = f.read(4 * 1024 * 1024) # 4MB chunk
            if not chunk:
                break
            
            append_data = {
                "command": "APPEND",
                "media_id": media_id,
                "segment_index": segment_id
            }
            files = {'media': chunk}
            
            res_append = requests.post(url, data=append_data, files=files, auth=auth)
            
            if res_append.status_code < 200 or res_append.status_code >= 300:
                print(f"âŒ APPEND Failed: {res_append.text}")
                raise Exception("Media Upload APPEND Failed")
            
            segment_id += 1

    # --- Step 3: FINALIZE ---
    print("ğŸ“¤ [v2] Upload Step 3: FINALIZE")
    finalize_data = {
        "command": "FINALIZE",
        "media_id": media_id
    }
    
    res_fin = requests.post(url, data=finalize_data, auth=auth)
    
    if res_fin.status_code < 200 or res_fin.status_code >= 300:
        print(f"âŒ FINALIZE Failed: {res_fin.text}")
        raise Exception("Media Upload FINALIZE Failed")
    
    # --- Step 4: STATUS CHECK (ã“ã“ã‚’å¼·åŒ–) ---
    print(f"â³ Waiting for media processing (ID: {media_id})...")
    
    # å¿µã®ãŸã‚æœ€åˆã«å¼·åˆ¶å¾…æ©Ÿ
    time.sleep(5)
    
    check_url = "https://upload.twitter.com/1.1/media/upload.json"
    status_params = {
        "command": "STATUS",
        "media_id": media_id
    }
    
    # æœ€å¤§10å› (ç´„30ç§’) ç¢ºèªã™ã‚‹ãƒ«ãƒ¼ãƒ—
    for i in range(10):
        res_status = requests.get(check_url, params=status_params, auth=auth)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆï¼ˆå³æ™‚å®Œäº†ï¼‰ã‚‚è€ƒæ…®
        if res_status.status_code != 200:
             # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãã€æƒ…å ±ãŒãªã„ã ã‘ãªã‚‰æˆåŠŸã¨ã¿ãªã—ã¦æŠœã‘ã‚‹æ‰‹ã‚‚ã‚ã‚‹ãŒã€å¿µã®ãŸã‚å¾…ã¤
             print(f"   Status check {i+1}: No info yet, waiting...")
             time.sleep(3)
             continue
             
        status_data = res_status.json()
        processing_info = status_data.get('processing_info', {})
        state = processing_info.get('state')
        
        print(f"   Status check {i+1}: {state if state else 'succeeded (completed)'}")
        
        if state == 'succeeded' or not state:
            print("âœ… Media processing completed!")
            return media_id
        
        elif state == 'failed':
            error = processing_info.get('error', {})
            raise Exception(f"Media processing failed: {error}")
            
        elif state == 'in_progress' or state == 'pending':
            wait_secs = processing_info.get('check_after_secs', 2)
            time.sleep(wait_secs)
        else:
            time.sleep(3)
            
    # ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã¦ã‚‚ã“ã“ã«æ¥ã‚‹å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã ãŒã€IDã¯è¿”ã™
    print("âš ï¸ Warning: Status check timed out, but proceeding...")
    return media_id

# ==========================================
# 3. è¨­å®šã‚¨ãƒªã‚¢ & ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
TARGET_AREAS = [
    ("æµ¦å®‰", "æµ¦å®‰ï¼ˆå¤¢ã®å³¶ãƒ»è‹¥æ´²ï¼‰"),
    ("å¤§é»’", "æ¨ªæµœï¼ˆã¿ãªã¨ã¿ã‚‰ã„ãƒ»å¤§é»’ï¼‰"),
    ("å¸‚åŸ", "åƒè‘‰ï¼ˆåƒè‘‰æ¸¯/å†…æˆ¿ï¼‰")
]

STATIONS = {
    "kawasaki": {"name": "å·å´äººå·¥å³¶", "lat": 35.49028, "lon": 139.83389, "file": "kawasaki_environment.xlsx"},
    "1goto": {"name": "1å·ç¯æ¨™", "lat": 35.53694, "lon": 139.95417, "file": "1goto_environment.xlsx"}
}

KNOWN_LOCATIONS = {
    "1å·ç¯æ¨™": (35.5369, 139.9542), "å·å´äººå·¥å³¶": (35.4903, 139.8339),
    "å·å´": (35.4903, 139.8339), "ç£¯å­": (35.4055, 139.6453),
    "æœ¬ç‰§": (35.4285, 139.6873), "å¤§é»’": (35.4487, 139.6945),
    "å¸‚åŸ": (35.5350, 140.0750), "æµ¦å®‰": (35.6400, 139.9417),
    "æ¤œè¦‹å·æ²–": (35.6108, 140.0233)
}
CANDIDATE_FACILITIES = ["æœ¬ç‰§", "å¤§é»’", "ç£¯å­", "å¸‚åŸ"]

MODELS_CONFIG = {
    "G1": {"model": "fish_catch_model_G1.pkl", "encoder": "label_encoders_G1.pkl"},
    "G2": {"model": "fish_catch_model_G2.pkl", "encoder": "label_encoders_G2.pkl"},
    "G3": {"model": "fish_catch_model_G3.pkl", "encoder": "label_encoders_G3.pkl"},
    "G4": {"model": "fish_catch_model_G4.pkl", "encoder": "label_encoders_G4.pkl"},
    "water": "sub/water_temp_model.pkl", "turbidity": "sub/turbidity_model.pkl",
    "salt": "sub/salt_model.pkl", "do": "sub/do_model.pkl"
}

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def get_angler_comment(row_data, g_cpue_dict):
    wind = row_data['é¢¨é€Ÿ(m/s)']
    rain = row_data.get('é™æ°´é‡(mm)', 0)
    temp_diff = row_data.get('å‰æ—¥æ°´æ¸©å·®', 0)
    total_cpue = row_data['â˜…ç·é‡£æœ(CPUE)']
    if wind >= 8.0: return "âš  å¼·é¢¨äºˆå ±ï¼å®‰å…¨ç¬¬ä¸€ã§æ’¤é€€ã‚‚å‹‡æ°—"
    if rain >= 5.0: return "â˜” æœ¬é™ã‚Šäºˆå ±ã€‚é›¨å…·å¿…é ˆã€è¶³å…ƒæ³¨æ„"
    if total_cpue >= 20.0: return "â˜…çˆ†é‡£è­¦å ±ï¼ã‚¯ãƒ¼ãƒ©ãƒ¼æº€ã‚¿ãƒ³ã®æº–å‚™ã‚’"
    if g_cpue_dict.get('G2', 0) >= 0.5: return "å¤§ç‰©ãƒãƒ£ãƒ³ã‚¹ï¼ãƒ«ã‚¢ãƒ¼ãƒ»æ³³ãŒã›ã§æ”»ã‚ã‚"
    if g_cpue_dict.get('G1', 0) >= 8.0: return "â— ã‚¢ã‚¸ãƒ»ã‚¤ãƒ¯ã‚·å›éŠï¼ã‚µãƒ“ã‚­ã§æ‰‹å …ã"
    if g_cpue_dict.get('G3', 0) >= 1.5: return "åº•ç‰©ãŒç†±ã„ï¼æŠ•ã’é‡£ã‚Šã§ã˜ã£ãã‚Šæ¢ã‚Œ"
    if temp_diff <= -0.5: return "æ°´æ¸©ä½ä¸‹ä¸­ã€‚æ´»æ€§ä½ã„ãªã‚‰æ·±å ´ãƒ»ãƒœãƒˆãƒ ã¸"
    if temp_diff >= 0.5: return "æ°´æ¸©ä¸Šæ˜‡ï¼æµ…å ´ã®é«˜æ´»æ€§ãªå€‹ä½“ã‚’ç‹™ãˆ"
    if total_cpue >= 10.0: return "å¥½æ¡ä»¶ï¼è‰²ã€…ãªé­šç¨®ãŒç‹™ãˆã‚‹ä¸€æ—¥"
    if total_cpue <= 3.0: return "æˆ‘æ…¢ã®å±•é–‹ã€‚æ½®ã®å¤‰ã‚ã‚Šç›®ã«é›†ä¸­ã—ã‚ˆã†"
    return "ã‚¨ãƒ³ã‚¸ãƒ§ã‚¤ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼ä¸€ç™ºé€†è»¢ã‚’ç‹™ãˆ"

def evaluate_cpue_total_scaled(val):
    if val >= 20.0: return "S (çˆ†é‡£)"
    if val >= 10.0: return "A (å¥½èª¿)"
    if val >= 4.0: return "B (æ™®é€š)"
    if val >= 1.2: return "C (æ¸‹ã„)"
    return "D (æ¿€æ¸‹)"

def match_features(model, available_data):
    try:
        if hasattr(model, 'feature_name_'): required_cols = model.feature_name_
        elif hasattr(model, 'feature_name'): required_cols = model.feature_name()
        else: required_cols = []
    except: required_cols = []
    
    if len(required_cols) == 0: return pd.DataFrame([available_data])
    input_data = {}
    for col in required_cols:
        val = available_data.get(col)
        if val is None:
            for k, v in available_data.items():
                if k in col or col in k: val = v; break
        input_data[col] = [val if val is not None else 0]
    return pd.DataFrame(input_data)

def get_coordinates(place_name):
    for key, val in KNOWN_LOCATIONS.items():
        if key in str(place_name): return val
    try:
        geolocator = Nominatim(user_agent="fishing_predictor_bot")
        loc = geolocator.geocode(place_name)
        if loc: return (loc.latitude, loc.longitude)
    except: pass
    return None

def calculate_moon_age(dt):
    base = datetime.datetime(2000, 1, 6, 12, 0)
    diff = dt - base
    return round((diff.total_seconds() / 86400) % 29.53058867, 1)

def get_weather_code_label(code):
    if code <= 1: return "æ™´ã‚Œ"
    if code <= 48: return "æ›‡ã‚Š"
    return "é›¨"

def fetch_weather_forecast_range(lat, lon, start_dt, end_dt):
    fetch_start = start_dt - datetime.timedelta(days=5)
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": lat, "longitude": lon, 
        "start_date": fetch_start.strftime("%Y-%m-%d"),
        "end_date": end_dt.strftime("%Y-%m-%d"), 
        "daily": "temperature_2m_mean,wind_speed_10m_max,precipitation_sum,pressure_msl_mean,weather_code",
        "timezone": "Asia/Tokyo", "wind_speed_unit": "ms"
    }
    try:
        res = requests.get(url, params=params, timeout=10)
        data = res.json()
        if "daily" in data: return pd.DataFrame(data["daily"])
    except: return None
    return None

def find_best_substitute(target_weather_row, date_str, candidates_weather_cache):
    best_facility = "æœ¬ç‰§"
    min_score = float('inf')
    t_wind = target_weather_row.get('wind_speed_10m_max', 0)
    t_temp = target_weather_row.get('temperature_2m_mean', 15)
    for facility in CANDIDATE_FACILITIES:
        df_cand = candidates_weather_cache.get(facility)
        if df_cand is None: continue
        cand_row = df_cand[df_cand['time'] == pd.to_datetime(date_str)]
        if len(cand_row) == 0: continue
        cand_row = cand_row.iloc[0]
        diff = abs(t_wind - cand_row['wind_speed_10m_max']) * 2.0 + abs(t_temp - cand_row['temperature_2m_mean'])
        if diff < min_score: min_score = diff; best_facility = facility
    return best_facility

def get_latest_marine_data(target_lat, target_lon):
    def calc_dist(lat1, lon1, lat2, lon2): return np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)
    dk = calc_dist(target_lat, target_lon, STATIONS["kawasaki"]["lat"], STATIONS["kawasaki"]["lon"])
    d1 = calc_dist(target_lat, target_lon, STATIONS["1goto"]["lat"], STATIONS["1goto"]["lon"])
    st = STATIONS["kawasaki"] if dk < d1 else STATIONS["1goto"]
    
    if not os.path.exists(st['file']):
        return None, None
    try:
        df = pd.read_excel(st['file'])
        lr = df.iloc[-1]
        vals = {"water_temp": lr['æ°´æ¸©(ä¸Šå±¤)(â„ƒ)'], "turbidity": lr['æ¿åº¦(ä¸Šå±¤)(NTU)'], "do": lr['DO(ä¸Šå±¤)(mg/L)'], "salt": lr['å¡©åˆ†(ä¸Šå±¤)(-)']}
        return vals, pd.to_datetime(lr.iloc[0])
    except: return None, None

def safe_encode(encoder, val):
    try: return encoder.transform([val])[0]
    except: return 0 

# --- ç”»åƒç”Ÿæˆé–¢æ•° ---
def generate_fishing_card(card_data_list, target_date_str):
    print("\nğŸ¨ äºˆå ±ã‚«ãƒ¼ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...")
    
    font_path = "ipaexg.ttf"
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        plt.rcParams['font.family'] = 'IPAexGothic'
    else:
        plt.rcParams['font.family'] = 'sans-serif'
            
    fig, ax = plt.subplots(figsize=(10, 6.5))
    fig.patch.set_facecolor('#f0f8ff')
    ax.set_facecolor('#f0f8ff')
    ax.axis('off')

    dt = datetime.datetime.strptime(target_date_str, "%Y-%m-%d")
    date_display = dt.strftime("%Y/%m/%d (%a)")
    
    plt.text(0.5, 0.93, 'æ±äº¬æ¹¾ é‡£æœäºˆæ¸¬AI', ha='center', va='center', fontsize=22, fontweight='bold', color='#003366')
    plt.text(0.5, 0.85, f'Target Date: {date_display}', ha='center', va='center', fontsize=13, color='#444444')

    y_positions = [0.65, 0.40, 0.15]
    colors = {'A (å¥½èª¿)': '#ffcccc', 'B (æ™®é€š)': '#fff5cc', 'C (æ¸‹ã„)': '#e6f2ff', 'D (æ¿€æ¸‹)': '#f0f0f0', 'S (çˆ†é‡£)': '#ff9999'}
    text_colors = {'A (å¥½èª¿)': '#cc0000', 'B (æ™®é€š)': '#996600', 'C (æ¸‹ã„)': '#003399', 'D (æ¿€æ¸‹)': '#666666', 'S (çˆ†é‡£)': '#cc0000'}

    for i, item in enumerate(card_data_list):
        if i >= 3: break
        y = y_positions[i]
        area_label = item['area_label']
        row_data = item['data']
        comment = item['ai_comment']
        
        rect = patches.FancyBboxPatch((0.05, y - 0.1), 0.9, 0.2, boxstyle="round,pad=0.02", linewidth=1, edgecolor='#cccccc', facecolor='white')
        ax.add_patch(rect)
        plt.text(0.1, y + 0.03, area_label, fontsize=16, fontweight='bold', color='#333333', va='center')
        judge = row_data['ç·åˆåˆ¤å®š']
        bg_c = colors.get(judge, '#ffffff')
        txt_c = text_colors.get(judge, '#000000')
        v_rect = patches.FancyBboxPatch((0.55, y - 0.08), 0.35, 0.16, boxstyle="round,pad=0.02", linewidth=0, facecolor=bg_c)
        ax.add_patch(v_rect)
        judge_short = judge.split(' ')[0]
        judge_jp = judge.split(' ')[1].replace('(', '').replace(')', '')
        plt.text(0.725, y + 0.03, f"{judge_short} {judge_jp}", ha='center', va='center', fontsize=20, fontweight='bold', color=txt_c)
        details = f"å¤©æ°—: {row_data['å¤©æ°—']} | é¢¨: {row_data['é¢¨é€Ÿ(m/s)']}m | æ°´æ¸©: {row_data['æ°´æ¸©(â„ƒ)']}â„ƒ | ç·åˆCPUE: {row_data['â˜…ç·é‡£æœ(CPUE)']}"
        plt.text(0.1, y - 0.05, details, fontsize=11, color='#555555', va='center')
        plt.text(0.725, y - 0.04, comment, ha='center', va='center', fontsize=11, fontweight='bold', color='#d9534f')

    plt.text(0.5, 0.02, 'Powered by Python & Fishing Forecast Model', ha='center', va='center', fontsize=10, color='#888888')
    plt.tight_layout()
    filename = 'fishing_forecast_card.png'
    plt.savefig(filename, dpi=150)
    plt.close()
    return filename

# ==========================================
# 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================
try:
    print("ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    models = {}; encoders = {}; 
    for key, path in MODELS_CONFIG.items():
        if isinstance(path, dict):
            if os.path.exists(path["model"]):
                models[key] = joblib.load(path["model"])
                encoders[key] = joblib.load(path["encoder"])
        else:
            if os.path.exists(path): models[key] = joblib.load(path)

    tomorrow = datetime.date.today() + datetime.timedelta(days=1)
    TARGET_DATE_STR = tomorrow.strftime("%Y-%m-%d")
    
    card_data_list = []
    
    for place_name, display_name in TARGET_AREAS:
        print(f"\nğŸš€ {place_name} ã®äºˆæ¸¬é–‹å§‹...")
        coords = get_coordinates(place_name)
        if not coords: continue

        current, last_marine_date = get_latest_marine_data(coords[0], coords[1])
        if current is None:
            current = {"water_temp": 12.0, "turbidity": 2.5, "salt": 31.5, "do": 9.5}
            last_marine_date = datetime.datetime.now() - datetime.timedelta(days=1)

        target_start_dt = pd.to_datetime(TARGET_DATE_STR)
        sim_start_dt = last_marine_date + datetime.timedelta(days=1)
        if target_start_dt < sim_start_dt: target_start_dt = sim_start_dt

        df_w = fetch_weather_forecast_range(coords[0], coords[1], sim_start_dt, target_start_dt)
        if df_w is None: continue
        
        c_cache = {}
        for f in CANDIDATE_FACILITIES:
            cc = get_coordinates(f)
            if cc:
                d = fetch_weather_forecast_range(cc[0], cc[1], sim_start_dt, target_start_dt)
                if d is not None: d['time'] = pd.to_datetime(d['time']); c_cache[f] = d

        df_w['time'] = pd.to_datetime(df_w['time'])
        target_row = df_w[df_w['time'].dt.strftime('%Y-%m-%d') == TARGET_DATE_STR]
        
        if not target_row.empty:
            row = target_row.iloc[0]
            date = row['time']
            w_label = get_weather_code_label(row['weather_code'])
            
            pool = {
                'æ°—æ¸©': row['temperature_2m_mean'], 'é¢¨é€Ÿ': row.get('wind_speed_10m_max', 0),
                'é™æ°´é‡': row.get('precipitation_sum', 0), 'æ°—åœ§': row.get('pressure_msl_mean', 1013),
                'æ—¥ä»˜': date.dayofyear, 'æ—¥ä»˜(365)': date.dayofyear, 'æœˆé½¢': calculate_moon_age(date),
                'å‰æ—¥ã®æ°´æ¸©': current['water_temp'], 'æ°´æ¸©': current['water_temp'],
                'å‰æ—¥ã®æ¿åº¦': current['turbidity'], 'æ¿åº¦': current['turbidity'],
                'å‰æ—¥ã®å¡©åˆ†': current['salt'], 'å¡©åˆ†': current['salt'], 'å‰æ—¥ã®DO': current['do'], 'DO': current['do'],
                'å¹³å‡æ°—æ¸©': row['temperature_2m_mean'], '5æ—¥å¹³å‡æ°—æ¸©': row['temperature_2m_mean']
            }
            try:
                pw = models['water'].predict(match_features(models['water'], pool))[0] if 'water' in models else current['water_temp']
                pt = models['turbidity'].predict(match_features(models['turbidity'], pool))[0] if 'turbidity' in models else current['turbidity']
                ps = models['salt'].predict(match_features(models['salt'], pool))[0] if 'salt' in models else current['salt']
                pd_val = models['do'].predict(match_features(models['do'], pool))[0] if 'do' in models else current['do']
                pt = max(0.1, pt)
                pool.update({'äºˆæ¸¬æ°´æ¸©': pw, 'æ°´æ¸©': pw, 'å‰æ—¥ã¨ã®æ°´æ¸©å·®': pw - current['water_temp'], 'æ¿åº¦': pt, 'å¡©åˆ†': ps, 'DO': pd_val})
            except: pw, pt, ps, pd_val = current.values()

            sub_place = find_best_substitute(row, TARGET_DATE_STR, c_cache)
            g1_total = 0
            fish_preds = {}
            g_cpue_sums = {}
            total_all_cpue = 0
            
            for g_name in ["G1", "G2", "G3", "G4"]:
                if g_name in models:
                    m, e = models[g_name], encoders[g_name]
                    pool['æ–½è¨­å'] = safe_encode(e['æ–½è¨­å'], sub_place)
                    pool['å¤©æ°—'] = safe_encode(e['å¤©æ°—'], w_label)
                    g_sum = 0
                    for fish in e['é­šç¨®'].classes_:
                        pool['é­šç¨®'] = safe_encode(e['é­šç¨®'], fish)
                        pred = max(0, m.predict(match_features(m, pool))[0])
                        fish_preds[fish] = pred
                        g_sum += pred
                    g_cpue_sums[g_name] = g_sum
                    total_all_cpue += g_sum
            
            grade = evaluate_cpue_total_scaled(total_all_cpue)
            result_row = {
                "æ—¥ä»˜": TARGET_DATE_STR, "å¤©æ°—": w_label, 
                "é¢¨é€Ÿ(m/s)": round(row.get('wind_speed_10m_max', 0), 1),
                "æ°´æ¸©(â„ƒ)": round(pw, 1), "å‰æ—¥æ°´æ¸©å·®": round(pw - current['water_temp'], 1),
                "ç·åˆåˆ¤å®š": grade, "â˜…ç·é‡£æœ(CPUE)": round(total_all_cpue, 1)
            }
            comment = get_angler_comment(result_row, g_cpue_sums)
            card_data_list.append({"area_label": display_name, "data": result_row, "ai_comment": comment})

    if card_data_list:
        # 1. ç”»åƒç”Ÿæˆ
        image_file = generate_fishing_card(card_data_list, TARGET_DATE_STR)
        
        # 2. æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Tweepyã‚’ä½¿ã‚ãšv2ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å©ã)
        media_id = upload_media_v2(image_file, consumer_key, consumer_secret, access_token, access_token_secret)
        
        print(f"âœ… Ready to tweet with Media ID: {media_id}")

        # 3. v2ã§ãƒ„ã‚¤ãƒ¼ãƒˆ (media_idã‚’æ·»ä»˜)
        client = tweepy.Client(
            consumer_key=consumer_key, consumer_secret=consumer_secret,
            access_token=access_token, access_token_secret=access_token_secret
        )
        
        tweet_text = f"""ğŸ“Š æ±äº¬æ¹¾é‡£æœäºˆæ¸¬ ({tomorrow.strftime('%m/%d')})

ã€é‡£è¡Œåˆ¤æ–­AIã€‘
æ˜æ—¥é‡£ã‚Šã«è¡Œã“ã†ã‹è¿·ã£ã¦ã„ã‚‹æ–¹ã¸AIãŒã‚¢ãƒ‰ãƒã‚¤ã‚¹!!
ç”»åƒã§è©³ç´°ã‚’ãƒã‚§ãƒƒã‚¯ğŸ‘‡

Webç‰ˆ: https://tokyo-bay-fishing-ai-ypd33onggtcjxnh69ryijz.streamlit.app/

#é‡£ã‚Š #æ±äº¬æ¹¾ #ã‚·ãƒ¼ãƒã‚¹ #ã‚¢ã‚¸ãƒ³ã‚°
"""
        client.create_tweet(text=tweet_text, media_ids=[media_id])
        print("âœ… ã‚«ãƒ¼ãƒ‰ç”»åƒä»˜ããƒ„ã‚¤ãƒ¼ãƒˆæˆåŠŸï¼ (v2 Manual Upload)")
    else:
        print("âŒ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    # GitHub Actionsã§ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦è½ã¨ã™ãŸã‚
    raise e
